DATABASE_URL="file:./dev.db"
ANTHROPIC_API_KEY=""
# Anthropic structured output recommendation:
# - Start with claude-sonnet-4-5 (best balance of reliability/speed/cost per Anthropic docs).
# - For maximum reasoning reliability (higher cost/latency), consider claude-opus-4-5.
# Note: no model guarantees zero schema errors; keep strict schema parsing + retries enabled.
ANTHROPIC_MODEL="claude-sonnet-4-5"
OLLAMA_BASE_URL="http://localhost:11434"
# Ollama structured output recommendation:
# - Prefer stronger instruction-following models locally (for example: gpt-oss:20b).
# - Avoid very small models for strict JSON schema tasks unless latency is the only goal.
# - Pull first: `ollama pull gpt-oss:20b`
OLLAMA_MODEL="llama3.2"
# Auto routing priority when providerPreference="auto":
# - local_first: try Ollama first, then Anthropic
# - hosted_first: try Anthropic first, then Ollama
LLM_AUTO_PRIORITY="local_first"
ANALYSIS_MAX_CONCURRENCY="4"
ANALYSIS_LLM_SCOPE="deep_only"
